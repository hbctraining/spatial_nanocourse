---
title: "Visium HD Analysis"
authors: "Alex Bartlett, Meeta Mistry, Noor Sohail, and Will Gammerdinger"
date: "February 26th, 2025"

format: 
  html: 
    math: mathjax

execute:
  warning: false
---

Approximate time: 2 hours and 45 minutes

# Learning Objectives

-   Describe the elements of the Seurat object and how they are generated
-   Visually inspect and compare spatial scRNA-seq data before and after filtering
-   Execute clustering workflows and visualize results on a tissue section
-   Annotate celltypes using both spatial and gene expression information

# Visium HD

## Mouse Brain Visium HD Dataset

The Visium HD platform is compatible with human and mouse fresh frozen, fixed frozen, and formalin-fixed paraffin-embedded (FFPE) tissue sections. For this lesson, we will be working with data from a **fresh frozen coronal section of a mouse brain** sample. The end goal of this exercise to annotate the cortical layers.

Visium HD matches the 6.5 × 6.5 mm imaging area of the earlier Visium platform but now achieves higher resolution by dividing that area into 11 million tiles. These 2µm x 2µm squares are arrayed in a continuous lawn across the entire capture area. The squares are each uniquely barcoded with an oligonucleotide and contain probes allowing for the detection of the full coding transcriptome. As such, Visium HD is categorized as a **sequencing-based technology**.

<p align="center">

![[10x Visium HD Spatial Gene Expression Manual](https://cdn.10xgenomics.com/image/upload/v1756507345/support-documents/CG000685_VisiumHD_GeneExpression_UserGuide_RevD.pdf)](../img/visium_hd_slide_graphic.png){width="800"}

</p>

The single-digit micron resolution is a big technological improvement over Visium's original ∼55μm spots. Because a single spot can include several cell types, the dataset is not straightforward to annotate. As a result, many methods model each spot as a mixture, assigning proportion values for each cell type within that spot. Each spot is represented as a piechart showing proportions of the cell type composition.

![[Ma, Y., et al. Nat Biotechnol (2022)](https://www.nature.com/articles/s41587-022-01273-7), Figure 1](../img/card_deconvolution.png){width="50%"}

While Visium HD reduces bin size to approach single-cell resolution, it is important to note that sequencing-based spatial transcriptomics captures RNA from a defined spatial area rather than from isolated cells. As a result, transcripts from multiple cells overlapping a bin can be pooled and sequenced together. This means that even small bins may contain mixed cellular signals, which has important implications for downstream analyses such as differential gene expression.

::: {.callout-note collapse="true"}
# Imaging-based technologies

These methods use fluoresence to quantify gene expression directly on a tissue section. They rely on fluoresence *in situ* hybridization (FISH) to measure the expression of a select panel of genes (chosen by the researcher) using **probes**. Since transcripts are detected within intact cells, expression values can be assigned to individual cells following cell segmentation.

Some popular imaging-based technologies include:

-   seqFISH
-   MERFISH
-   Xenium
:::

## Preprocessing Data with SpaceRanger

Sequencing facilities often output scRNA-seq data, including spatial scRNA-seq data, in FASTQ format. Because this is Visium HD data from 10X Genomics, we used their proprietary pre-processing software [Space Ranger](https://www.10xgenomics.com/support/software/space-ranger/latest) to process the FASTQ files into a count matrix and other images. The `spaceranger count` command aligns the reads in the FASTQ files against a reference genome and provides their spatial location using the oligonucleotide barcode.

Note that Space Ranger requires a Linux system with at least 32 cores, 64GB of RAM, and 1TB of disk space.

::: {.callout-note collapse="true"}
# Example `spaceranger count` command

A sample command for running `spaceranger count` is:

```{bash}
#| label: spaceranger_example
#| eval: false
spaceranger count --id=hd_count \
   --transcriptome=/path/to/refdata-gex-GRCh38-2020-A \
   --fastqs=/path/to/fastq \
   --probe-set=/path/to/Visium_Human_Transcriptome_Probe_Set_v2.0_GRCh38-2020-A.csv \
   --slide=H1-YD7CDZK \
   --area=A1 \
   --cytaimage=/path/to/CAVG10539_2023-11-16_14-56-24_APPS115_H1-YD7CDZK_A1_S11088.tif \
   --image=/path/to/APPS115_11088_rescan_01.btf \
   --create-bam=false
```
:::

When `spaceranger count` completes successfully, it will generate a variety of outputs (seen below), which will enable the analyst to perform further analysis in R/Python or using the proprietary Loupe browser from 10X Genomics. A good starting point is to take a look at the QC of the sample in the web summary, which we have provided in `reports/` folder that you downloaded.

![[10x Space Ranger Documentation](https://www.10xgenomics.com/support/software/space-ranger/2.1/tutorials/count-cytassist-tma)](../img/spaceranger_output.svg){width="600%"}

In the Visium HD assay, Space Ranger aggregates transcript counts into square spatial bins of different sizes, typically:

-   2µm x 2µm
-   8µm x 8µm
-   16µm x 16µm

Having access to 2μm bins, along with matched high-resolution tissue morphology, provides a great opportunity to reconstruct single cells from the data. However, because the 2µm x 2µm bins (and even the 8µm x 8µm bins) are very small, there is a potential for very **little biological signal to be captured per bin**. Additionally, the sheer number of bins at these higher resolutions can substantially increase computational demands in terms of memory usage and processing time.

For this lesson, we will use the **16µm x 16µm bins of the cropped Visium HD slide** to run locally on laptops.

## NGS-based Spatial Transcriptomics Analysis Workflow

The overarching steps for analyzing a sequencing-based transcriptomics dataset is as follows:

<p align="center">

<img src="../img/Full_workflow.png" width="800"/>

</p>

# Setting Up in R

## Downloading the Data

For this module, we will be working within an RStudio project. In order to follow along you should have **downloaded the R project**.

::: callout-important
# Where to download the data

If you haven't done this already, the project is located in "Dataset for workshop" -\> "Day 1- NGS-based- VisiumHD" in the course DropBox.
:::

Once downloaded, you should see a file called `visiumHD_nanocourse.zip` on your computer (likely, in your `Downloads` folder).

1.  **Unzip this file.** This will result in a folder of the same name.

2.  **Move the folder to the location on your computer where you would like to perform the analysis.**

3.  **Open up the folder.** The contents will look like the screenshot below:

    <p align="center">

    <img src="../img/proj_screenshot.png" width="500"/>

    </p>

4.  **Locate the `.Rproj file` and double-click on it.** This will open up RStudio with the "visiumHD_nanocourse" project loaded.

5.  **Open a new Rscript file.**

6.  **Start with some comments to indicate what this file is going to contain:**

```{r}
#| label: header
# February 2026
# Spatial Transcriptomics
# Visium HD Demo
# Harvard Chan Bioinformatics Core
```

7.  **Save the Rscript in the `code` folder as `visiumHD.R`.** Your working directory should look something like this:

    <p align="center">

    <img src="../img/Code_folder.gif" width="700"/>

    </p>

## Loading Libraries

Next, we will need to be sure to load the libraries that we will be using:

```{r}
#| label: load_libraries
# Load libraries
library(tidyverse)
library(patchwork)
library(Seurat)
library(qs)
library(SeuratWrappers)
library(Banksy)
library(quadprog)
library(spacexr)

# Increases the size of the default vector
options(future.globals.maxSize= 2000000000)
```

# Creating the Seurat Object

The Seurat object is a custom list-like object that has well-defined spaces to store specific information/data for single-cell experiments, including spatial experiments and Visium HD.

The Seurat package provides a function `Load10X_Spatial()` to easily create a Seurat object from the output of Space Ranger. The `Load10X_Spatial` function takes the feature matrix and low-resolution tissue image from the Space Ranger output and generates a Seurat object containing both gene-level counts and spatial information.

**We will not have you run this code**, as this can take some time and the SpaceRanger output files are too large to share. Instead, you will load the pre-made Seurat object.

::: {.callout-note collapse="true"}
# R code used to create the Seurat object

```{r}
#| label: loax10x_spatial_example
#| eval: false
localdir <- '../path/to/spaceranger/outs/'

#Load the raw feature matrix
object <- Load10X_Spatial(data.dir = localdir,
                          filename = 'raw_feature_bc_matrix.h5',
                          bin.size = 16)
```
:::

Let's read in the Seurat object and talk about some very basic slots that we will be accessing.

```{r}
#| label: qread_object
# Load in Seurat object
object <- readRDS("data_processed/MsBrain_FF-A1_subset_updated.rds")
```

We can print the Seurat object by calling the `object` we created in the console:

```{r}
#| label: callout_object_1
object
```

Now we can examine its major features, which we will add to and alter throughout the lesson:

<p align="center">

<img src="../img/Base_seurat_object_labelled.png" width="1000"/>

</p>

::: callout-tip
# Exercise

There are 3 things about our Seurat object printout that would be different if we were using the 8µm x 8µm binning instead of the 16µm x 16µm binning. What are these three differences?

<p align="center">

<iframe src="https://hcbc.connect.hms.harvard.edu/Spatial_resolution_question_1/?showcase=0" width="600px" height="250px" data-external="1">

</iframe>

</p>
:::

# Quality Control

The goal of quality control is to retain only high-quality bins. Once we clean the data, it becomes easier to identify distinct celltype populations when we cluster the bins.

<p align="center">

<img src="../img/QC_workflow.png" width="800"/>

</p>

With Visium HD data, the main challenge is **delineating bins that are poor quality from bins containing reads from less complex cells**. If you expect a particular cell type in your dataset to be less transcriptionally active as compared other cell types in your dataset, the bins underneath this cell type will naturally have fewer detected genes and transcripts. However, having fewer detected genes and transcripts can also be a technical artifact and not a result of biological signal.

Various metrics can be used to filter low-quality bins from high-quality ones, including:

| Metric                          | Description                                                                                                                                                                                                                                                                                                                                                                                                               |
|---------------------|---------------------------------------------------|
| UMI counts per bin (nCounts)    | Number of unique transcripts (UMIs) detected per bin. Because the bins are very small, this number is lower than what is typically observed in non-spatial scRNA-seq data.                                                                                                                                                                                                                                                |
| Genes detected per bin (nGenes) | Number of unique genes detected per bin. Because the bins are very small, this number is lower than what is typically observed in non-spatial scRNA-seq data.                                                                                                                                                                                                                                                             |
| Complexity (novelty score)      | A measure of how diverse the captured transcripts are within a bin. If there are many captured transcripts (high nUMI) but a low number of genes, this suggests repeatedly sequencing a small set of genes (low complexity). Good-quality bins generally have a complexity score above 0.80. The complexity score is computed as: <br> $\text{Complexity Score} = \dfrac{\log_{10}(\text{Number of Genes})}{\log_{10}(\text{Number of UMIs})}$. |
| Mitochondrial counts ratio      | Identifies bins with potential mitochondrial contamination from dead or dying cells. Poor-quality bins are typically defined as those with a mitochondrial ratio above 0.2, unless high mitochondrial content is expected for the sample. This ratio is computed as: <br> $\text{Mitochondrial Ratio} = \dfrac{\text{Number of reads aligning to mitochondrial genes}}{\text{Total reads}}$.                              |

Let's take a quick look at the data and make a decision on whether we need to apply any filtering. We will examine the distributions of UMI counts per bin and genes detected per bin to determine reasonable thresholds for those metrics to implement during QC filtering.

## Pre-filtering

In order to create some plots, we will first need to create a metadata dataframe using this command:

```{r}
#| label: object_meta
object_meta <- object@meta.data
```

Now we can plot the number of UMIs (nCount) and the number of genes (nFeature) side-by-side. We expect to see a bimodal distribution for both of the plots, with one peak representing bins of lower-quality cells with fewer genes/UMIs and another peak representing bins that contain healthy cells with more genes and UMIs. Ideally, the peak representing lower-quality and dying cells is small and the peak representing healthy cells is large.

```{r}
#| label: pre_qc_density
# Create a plot for nUMI
dist_counts_before <- object_meta %>%
  ggplot(aes(x=nCount_Spatial.016um)) +
  geom_density(alpha = 0.2) +
  scale_x_log10() +
  theme_classic() +
  ylab("Cell density") +
  xlab("Number of UMIs per bin") +
  ggtitle('Pre-QC UMIs/Bin') +
  theme(plot.title = element_text(hjust = 0.5))

# Create a plot for nGene
dist_features_before <- object_meta %>%
  ggplot(aes(x=nFeature_Spatial.016um)) +
  geom_density(alpha = 0.2) +
  scale_x_log10() +
  theme_classic() +
  ylab("Cell density") +
  xlab("Number of genes per bin") +
  ggtitle('Pre-QC Genes/Bin') +
  theme(plot.title = element_text(hjust = 0.5))

dists_before <- dist_counts_before | dist_features_before
dists_before
```

::: callout-tip
# Exercise

Using the distribution plots in the app below, what do you think would be good minimum thresholds for nGene and nUMI?

<p align="center">

<iframe src="https://hcbc.connect.hms.harvard.edu/Spatial_threshold_question_2/?showcase=0" width="800px" height="410px" data-external="1">

</iframe>

</p>
:::

## Post-Filtering

We will apply very minimal filtering here, with nUMI \> 100 and nGene \> 100. It has been shown that low expression can be biologically meaningful in a spatial context so we won't be as stringent as we normally are with a scRNA-seq analysis.

```{r}
#| label: subset_object
# Create a filtered object
object_filt <- subset(object, 
                      subset = (nCount_Spatial.016um > 100) & 
                               (nFeature_Spatial.016um > 100))
```

Now, we can create similar plots with filtered data. As expected, we see that the small left peak in the distribution has vanished, which leaves the higher quality bins that are the majority of the data.

```{r}
#| label: post_filt_qc_density
# Create a new metadata data frame from filtered data
object_filt_meta <- object_filt@meta.data

# Plot nUMI
dist_counts_after <- object_filt_meta %>%
  ggplot(aes(x=nCount_Spatial.016um)) +
  geom_density(alpha = 0.2) +
  scale_x_log10() +
  theme_classic() +
  ylab("Cell density") +
  xlab("Number of UMIs per bin") +
  ggtitle('PostQC UMIs/Bin') +
  theme(plot.title = element_text(hjust = 0.5))

# Plot nGene
dist_features_after <- object_filt_meta %>%
  ggplot(aes(x=nFeature_Spatial.016um)) +
  geom_density(alpha = 0.2) +
  scale_x_log10() +
  theme_classic() +
  ylab("Cell density") +
  xlab("Number of genes per bin") +
  ggtitle('PostQC Genes/Bin') +
  theme(plot.title = element_text(hjust = 0.5))

# Combine plots side-by-side
dists_after <- dist_counts_after | dist_features_after
dists_after
```

::: callout-tip
# Exercise

How many cells did we filter out with these thresholds?
:::

## Visualizing Counts Data

We can visualize the number of UMIs and gene counts per bin, both as a distribution and layered on top of the tissue image. Let's start with a violin plot to look at the distribution of UMI counts and gene counts. The input is our post-filtered dataset.

```{r}
#| label: vln_qc
# Violin plot of UMI counts
vln_counts_after <- VlnPlot(object_filt, 
                            features = "nCount_Spatial.016um", 
                            pt.size = 0, 
                            group.by = 'orig.ident') + 
  NoLegend() + scale_y_log10() + ggtitle('nUMI') + xlab('') + ylim(c(100, 15000))

# Violin plot of gene counts
vln_features_after <- VlnPlot(object_filt, 
                            features = "nFeature_Spatial.016um", 
                            pt.size = 0, 
                            group.by = 'orig.ident') + 
  NoLegend() + scale_y_log10() + ggtitle('nGene') +  xlab('') + ylim(c(100, 15000))


# Plot both side by side
vln_counts_after | vln_features_after
```

We see that both distributions have a similar peak but the nUMI distribution has a much longer tail. This is expected, because while the small physical size of the bins means that most genes will be detected only once or twice, a minority of bins under very transcriptionally active cells may exhibit multiple transcripts of the same gene.

Next, we can look at the same metrics and the distribution on the actual image itself. Note that there are many spots with few counts, in part due to low cellular density or cell types with low complexity within certain tissue regions.

```{r}
#| label: SpatialFeaturePlot_qc
# Visualizing UMI count across the image
image_counts <- SpatialFeaturePlot(object_filt, 
                                   feature = 'nCount_Spatial.016um', 
                                   pt.size.factor = 8)

# Visualizing gene count across the image
image_features <- SpatialFeaturePlot(object_filt, 
                                     features = "nFeature_Spatial.016um", 
                                     pt.size.factor = 8) 

# Plot the two side-by-side
image_counts | image_features
```

# Normalize Data

Normalization is an important step for making expression counts comparable across genes and/or samples. We note that optimal normalization stategies for spatial transcriptomics data are still being developed and evaluated. In particular, [Bhuva et. al](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-024-03241-7) tested a number of normalization techniques and found that normalizing by the total number of transcripts detected per bin can negatively impact spatial domain identification, as variation in total detection can represent geniune biological structure rather than technical noise.

We are cognizant of this, but as discussed earlier, it can be challenging to determine whether low transcript counts in a given bin arise from biological differences or from technical factors such as capture efficiency. In the absence of normalization, bins with low overall signal can strongly influence clustering, regardless of whether the cause is technical or biological. For this reason, we apply a **standard log-transformed library size normalization** to our data to mitigate extreme differences in total counts and stabilize the data for downstream analysis.

```{r}
#| label: NormalizeData
object_filt <- NormalizeData(object_filt, assay = 'Spatial.016um')
object_filt
```

And we can now see there is a new `data` layer in the Seurat object.

<p align="center">

<img src="../img/Seurat_object_with_normalized_data_labelled.png" width="700"/>

</p>

# Unsupervised Clustering

The authors of the Seurat package recommend the Seurat v5 sketch clustering workflow because it exhibits improved performance for large datasets, particularly for identifying rare and spatially-restricted groups. Sketch-based analyses aim to "subsample" large datasets in a way that preserves rare populations.

<p align="center">

<img src="../img/Standard_clustering.png" width="800"/>

</p>

## Highly Variable Genes (HVGs)

We will start by defining a set of highly variable genes. *Note that this is being done on all bins within our Seurat object.* Using this list of genes will help us to quantify the variability and similarity between bins. Essentially, we are looking at genes with high levels of variance while also accounting for the average expression. In doing so, we get a list of genes ranked by how much they change across different cell populations.

```{r}
#| label: filt_FindVariableFeatures
object_filt <- FindVariableFeatures(object_filt)
object_filt
```

When we examine our Seurat object, we can see that `FindVariableFeatures()` has added 2,000 variable features.

<p align="center">

<img src="../img/Seurat_object_variable_features_labelled.png" width="700"/>

</p>

And we can see what the top variables features are for the entire dataset. We would anticipate that these genes correspond to the celltypes in our dataset.

```{r}
VariableFeatures(object_filt)[1:15]
```

If we look into the gene [Ttr](https://www.genecards.org/cgi-bin/carddisp.pl?gene=TTR), we can learn that it is "highly expressed in choroid plexus epithelial cells". We can visualize the expression of Ttr on the slide to see if we can already identify which cells belong to the choroid plexus.

```{r}
SpatialFeaturePlot(object_filt, "Ttr", pt.size.factor = 7)
```


If continue looking at the top variable genes, we would hope to see more genes with experimental or celltype specific relevance.

## Sketch Downsampling

Next, we select 10,000 cells and create a new sub-sampled "sketch" assay using the `SketchData()` function. The function takes a normalized single-cell dataset containing a set of variable features and returns a Seurat object with a new assay (sketch), which consists of 10,000 bins selected based on a "leverage score" for each bin.

The leverage score reflects the magnitude of the bin's contribution to the gene-covariance matrix, and its importance to the overall dataset, with rare populations earning a higher leverage score. As a result, the 10,000 bins we selected for with the sketch oversamples rare populations, retaining the biological complexity of the sample, while drastically compressing the dataset.

```{r}
# we select 10,000 cells and create a new 'sketch' assay
object_filt <- SketchData(
  object = object_filt,
  assay = 'Spatial.016um',
  ncells = 10000,
  method = "LeverageScore",
  sketched.assay = "sketch"
)

object_filt
```

We can see that there are four major changes that have taken place in our Seurat object:

-   The number of features in the second line has doubled, because we have added a new assay
-   Accordingly, the number of assays has increased from one to two
-   The active assay has changed from `Spatial.016um` to `sketch`
-   There is a new line listing additional assays that exist in the Seurat object

<p align="center">

<img src="../img/Seurat_object_sketch_labelled.png" width="700"/>

</p>

We can also see that the leverage score has been added as a column to the metadata of our object.

```{r}
#| label: view_metadata_1
#| eval: false
View(object_filt@meta.data)
```

```{r}
#| label: dt_metadata_1
#| echo: false
library(DT)
datatable(object_filt@meta.data %>% head(5))
```

## Clustering Workflow

Next, we will perform a standard clustering workflow on our sketch of 10,000 cells:

| Function                 | Description                                                                                                                                                          |
|----------------------|--------------------------------------------------|
| `FindVariableFeatures()` | As before, this generates a list of highly variable genes.                                                                                                           |
| `ScaleData()`            | Highly variable genes will be confounded with the most highly expressed genes, so we need to adjust for this by scaling and centering the normalized counts.         |
| `RunPCA()`               | Perform a principal component analysis using our scaled data and variable genes. This will emphasize variation in gene expression as well as similarity across bins. |
| `FindNeighbors()`        | Determine the Euclidean distance between bins in PCA space.                                                                                                          |
| `FindClusters()`         | Iteratively group bins together based on neighborhood distances. Higher resolution will yield more groups.                                                           |

::: {.callout-note collapse="true"}
# scRNA-seq workflow

These steps are all a part the standard single-cell RNA-seq workflow. For more detailed information on what each of these steps are, we have a [scRNA workshop](https://hbctraining.github.io/Intro-to-scRNAseq/) that detailed each of these steps shown in the following workflow and considerations that should be made:

<p align="center">

<img src="../img/sc_workflow_2022.jpg" width="700"/>

</p>
:::

### Variable Features

We will calculate the HVGs to use as inputs for Principal Component Analysis (PCA) for the next step. We can also visualize each gene's average expression across the bins on the x-axis and variance on the y-axis.

```{r}
#| label: sketch_FindVariableFeatures
#| fig-width: 10
object_filt <- FindVariableFeatures(object_filt)

# Identify the 15 most highly variable genes
ranked_variable_genes <- VariableFeatures(object_filt,
                                          assay = "sketch")
top_genes <- ranked_variable_genes[1:15]

# Plot the average expression and variance of these genes
# With labels to indicate which genes are in the top 15
p <- VariableFeaturePlot(object_filt)
LabelPoints(plot = p, points = top_genes, repel = TRUE)
```

::: callout-tip
# Exercise

You may notice that the top variable features are slightly different from when we calculated them previously. Why might that be the case?
:::

### PCA

Principal Component Analysis (PCA) is a technique used to emphasize both variation and similarity to reveal strong patterns within a dataset. It is one of the methods used for *dimensionality reduction*. PCA calculates values that represent how similar bins are to one another.

We would expect that bins with similar gene expression will have similar PC values. For example, we would anticipate that two Fibroblast cells would have comparable gene expression - which would result in similar scores in their principal components. This is why many of the downstream steps in the remainder of this lesson use PCA as the input.

```{r}
#| label: sketch_RunPCA
object_filt <- ScaleData(object_filt)
object_filt <- RunPCA(object_filt, assay = "sketch", 
                      reduction.name = "pca.sketch")
```

We can explore the PCs using heatmaps to explore how each principal component (PC) relates to gene expression. For each PC, we show the genes with the strongest positive and negative loadings (with bins ordered by PC scores). This lets us see whether the genes that drive a given PC separate different cell types, as indicated by contrasting expression patterns between bins with high (positive) and low (negative) PC scores.

```{r}
# Explore heatmap of PCs 
DimHeatmap(object_filt, 
           reduction = "pca.sketch",
           dims = 1:6, 
           cells = 500, 
           balanced = TRUE)
```

### Neighborhoods and Clusters

In order to properly group our bins together, we first construct a K-nearest neighbor (KNN) graph based on the PCA space; where edges are drawn between bins with similar gene expression profiles. Then, we refine edge weights between any two bins based on shared overlap within their local neighborhoods.

From these neighborhoods, the `FindClusters()` function iteratively groups bins together using the Louvain algorithm. The size of the clusters is determined by the `resolution` parameter. Higher resolution values produce a larger number of smaller clusters, which big datasets often require. Typically, you would test multiple different cluster resolutions to find a resolution that represents the different cell states in the dataset.

Here we are going to use `resolution = 0.65`.

```{r}
#| label: scrna_workflow_sketch
# K-nearest neighbors
object_filt <- FindNeighbors(object_filt, assay = "sketch", 
                             reduction = "pca.sketch", dims = 1:50)

# Louvain clustering
object_filt <- FindClusters(object_filt, 
                            cluster.name = "seurat_cluster.sketched", 
                            resolution = 0.65)
```

We can also see how many bins belong to each one of our clusters:

```{r}
ggplot(object_filt@meta.data) +
  geom_bar(aes(x = seurat_cluster.sketched, 
               fill = seurat_cluster.sketched)) +
  theme_bw() + NoLegend()
```

::: callout-tip
# Exercise

Why are there so many `NA` values in our results?
:::

### UMAP

Finally, let's construct a UMAP embedding using the principal components as input. UMAP is a nonlinear dimensionality reduction method that preserves local neighbourhood structure, placing bins that are similar in high-dimensional expression space together in a low-dimensional representation. This is useful for visualizing our newly calculated clusters. We observe clear separation between bins assigned to different clusters, which is sign that our clustering captures biologically distinct cell types or states.

```{r}
#| label: filt_sketch_umap
object_filt <- RunUMAP(object_filt, reduction = "pca.sketch", 
                       reduction.name = "umap.sketch", return.model = T, 
                       dims = 1:50)

# Plot UMAP
DimPlot(object_filt, reduction = "umap.sketch", 
        label = T, cols = 'polychrome') + 
  ggtitle("Sketched clustering") + NoLegend()
```

We can also examine our object after these manipulations and note the additions of the `scale.data` layer as well as the sketch PCA and UMAP dimensional reductions.

```{r}
object_filt
```

<p align="center">

<img src="../img/Seurat_object_scale_data_PCA_UMAP_labelled.png" width="700"/>

</p>

## Project Clusters Back to Entire Dataset

Now that we have our clusters and dimensional reductions from our sketched dataset, we next need to extend these to the full dataset. The `ProjectData` function projects all the bins in the dataset (the `Spatial.016um` assay) into the low dimensional space learned from the `sketch` assay.

```{r}
#| label: ProjectData
object_filt <- ProjectData(
  object = object_filt,
  assay = "Spatial.016um",
  full.reduction = "full.pca.sketch",
  sketched.assay = "sketch",
  sketched.reduction = "pca.sketch",
  umap.model = "umap.sketch",
  dims = 1:50,
  refdata = list(seurat_cluster.projected = "seurat_cluster.sketched")
)
object_filt
```

The `ProjectData` function uses the sketch PCA and UMAP to return a Seurat object that includes:

-   **Dimensional reduction (PCA)** - The `full.pca.sketch` dimensional reduction extends the PCA reduction on the sketched cells to all bins in the dataset
-   **Dimensional reduction (UMAP)** - The `full.umap.sketch` dimensional reduction extends the UMAP reduction on the sketched cells to all bins in the dataset
-   **Cluster labels** - The `seurat_cluster.projected` column in the object metadata now labels all cells in the dataset with one of the cluster labels derived from the sketched cells

We can now see the additional full-dataset reductions in the object.

<p align="center">

<img src="../img/Seurat_object_projected_data_labelled.png" width="700"/>

</p>

Note that a score for the projection of each bin will be saved as a column in the metadata. If we view the metadata again, we'll see that the `seurat_cluster.sketched` column contains many NA values because the clusters were only calculated for 10,000 bins.

```{r}
#| eval: false
View(object_filt@meta.data)
```

```{r}
#| echo: false
library(DT)
datatable(object_filt@meta.data %>% head(5))
```

### Visualizing the Projected Clusters on UMAP

We can now visualize our clusters from the projected assignments. The UMAP plot contains more points, as expected because we are now visualizing the full dataset rather than our 10,000 bin sketch. Nonetheless, we can see that the full dataset is still well-represented by the projected dimensional reduction and clustering.

```{r}
# switch to full dataset assay
DefaultAssay(object_filt) <- "Spatial.016um"

# Change the idents to the projected cluster assignments
Idents(object_filt) <- "seurat_cluster.projected"

# Plot the UMAP
DimPlot(object_filt, reduction = "full.umap.sketch", label = T, 
        raster = F, cols = 'polychrome') +
  ggtitle("Projected clustering") + NoLegend()
```

### Visualizing Projected Clusters on the Image

We can use the `SpatialDimPlot()` function to see the clusters superimposed on our spatial slide. We will also set the color palette and convert the cluster assignments to a factor so they are ordered numerically rather than lexicographically in the figure.

```{r}
# Sort clusters so they get listed in numerical order
order <- object_filt$seurat_cluster.projected %>%
  unique() %>% as.numeric() %>%
  sort() %>% as.character()

object_filt$seurat_cluster.projected <- factor(object_filt$seurat_cluster.projected,
                                               levels = order)

# Create color palette
color_pal <- Seurat::DiscretePalette(n = length(order),
                                     palette = "polychrome")
names(color_pal) <- order

# Visualize clusters on slide
image_seurat_clusters <- SpatialDimPlot(object_filt, 
                                        group.by = 'seurat_cluster.projected', 
                                        pt.size.factor = 8, cols = color_pal) +
  guides(fill=guide_legend(ncol=2))

image_seurat_clusters
```

# Spatially-informed Clustering

Recall that our aim is to **annotate the cortical layers** in our dataset. These layers are well defined by their spatial location so it makes sense to utilize a tool that allows us to pick up signal in spatially restricted regions.

[BANKSY](https://www.nature.com/articles/s41588-024-01664-3) is another method used to perform clustering. Unlike Louvain and Leiden clustering algorithms (`FindClusters()`), BANKSY takes into account both an individual bin's expression profile as well as the mean and the gradient of gene expression levels in a bin's broader neighborhood. This makes it valuable for identifying and defining spatial regions of interest.

We use the `RunBanksy` function to create a new `BANKSY` assay based on the 4,000 most highly variable features, which can be used for dimensionality reduction and clustering. Two parameters of importance are:

-   `k_geom` - Number of bins to consider for the local neighborhood. Larger values will yield larger domains.
-   `lambda` - Influence of the neighborhood. Larger values yield more spatially coherent domains. The authors recommend using 0.8 to identify broader spatial domains.

```{r}
#| label: run_banksy
# Run Banksy
object_filt <- RunBanksy(object_filt, lambda = 0.8, verbose = T,
                         assay = 'Spatial.016um', slot = 'data', k_geom = 50)
object_filt
```

We can see the new BANKSY **assay** in our object:

<p align="center">

<img src="../img/Seurat_object_BANKSY_labelled.png" width="700"/>

</p>

::: {.callout-note collapse="true"}
# What is BANKSY doing under the hood?

BANKSY calculates two different scores:

-   Weighted mean expression of genes in a spatial neighborhood
-   "Azimuthal gabor filter", which is the "gradient of gene expression in each cell's neighborhood"

In doing so, BANKSY is able to be flexible and not limit celltypes to only be located nearby one another. Even more than that, there is our `lambda` value that scales the weight of each of these matrices to give the user flexibility in how strongly the spatial weights should impact clustering.

These new values are then **concatenated to the counts matrix** before running the remainder of a standard scRNA-seq workflow (dimensionality reduction, clustering, integration, etc.). Therefore, the final output of this algorithm is a modulated counts matrix that includes weighted scores that correspond to spatial domains.

At this point, we have calculated our BANKSY matrix that includes the weighted scores based upon the `lambda` value. We can see this more clearly if we investigate the `Features()` that exist in our BANKSY assay. The first few features are our original counts matrix with genes:

```{r}
# First genes in seurat object
Features(object_filt) %>% head()
```

Where we also have "new features" that are our lambda scaled neighborhood weighted values, with an appended `*.m0` to make the distinction clear.

```{r}
# Last genes in seurat object
Features(object_filt) %>% tail()
```
:::

And so with our newly calculated BANKSY matrix, we can perform a simplified clustering workflow (PCA, neighbors, clustering):

```{r}
# PCA
object_filt <- RunPCA(object_filt, assay = "BANKSY", 
                      reduction.name = "pca.banksy", 
                      features = rownames(object_filt), npcs = 30)
# kNN
object_filt <- FindNeighbors(object_filt, reduction = "pca.banksy", 
                             dims = 1:30)
# Clustering
object_filt <- FindClusters(object_filt, cluster.name = "banksy_cluster",
                            resolution = 0.5)
```

Let's visualize the BANKSY clusters alongside the Louvain clusters (`seurat_cluster.projected`) for a side-by-side comparison:

```{r}
# Get unique clusters for color palette
clusters <- object_filt$banksy_cluster %>%
  unique() %>% as.numeric() %>%
  sort() %>% as.character()

color_pal <- Seurat::DiscretePalette(n = length(clusters),
                                    palette = "polychrome")
names(color_pal) <- clusters

# Visualize BANKSY clusters
image_banksy_clusters <- SpatialDimPlot(object_filt, group.by = "banksy_cluster",
                                        pt.size.factor = 7, cols = color_pal)

image_seurat_clusters | image_banksy_clusters
```

We can see that the BANKSY clusters are more spatially-restricted (or compact) than the Seurat Louvain clusters, as expected. Additionally BANKSY clusters are less noisy, likely due to the smoothing effect of using a bin's spatial neighborhood when assigning cluster labels.

::: callout-tip
# Exercise

If we had run BANKSY with `lambda = 0.2`, as recommended for cell type clustering instead of `lambda = 0.8` for spatial domain clustering, what do you think would happen?

::: {.callout-tip collapse="true"}
# BANKSY using a lambda value of 0.2

The resultant clusters would be less spatially restricted (in other words less compact and more distributed throughout the image) and more similar to our Seurat clustering. Below is a figure using `lamba = 0.2` in BANKSY rather than `lamba = 0.8`.

<p align="center">

<img src="../img/banksy_clustering_lambda_0.2.png" width="450"/>

</p>
:::
:::

Ultimately the aim here is to annotate the layers in the isocortex. Therefore we must ask ourselves which of our clustering resolutions best represent the layers in the dataset. Here, having known marker genes for the various layers is crucial in making that decision.

For example, `Cux2` is a good marker gene for identifying Layers 2/3 of the isocortex. We can roughly identify this layer in on our slide by looking at the expression pattern:

```{r}
DefaultAssay(object_filt) <- "Spatial.016um"
SpatialFeaturePlot(object_filt, "Cux2", pt.size.factor = 7,
                   image.alpha = 0.5)
```

Which means that we can now assess which clustering algorithm is able better able to identify this population of cells. Ideally you would have more than one gene to make this justification. When we look at the expression scores for each cluster, we can see that Layers 2/3 are being represented in cluster 2 of BANKSY clusters and in cluster 7 of the Louvain/seurat clustering.

```{r}
Idents(object_filt) <- "banksy_cluster"
p1 <- VlnPlot(object_filt, "Cux2", pt.size = 0) +
  ggtitle("BANKSY clusters: Cux2") + NoLegend()

Idents(object_filt) <- "seurat_cluster.projected"
p2 <- VlnPlot(object_filt, "Cux2", pt.size = 0) +
  ggtitle("Louvain Clusters: Cux2") + NoLegend()

p1 + p2
```



::: {.callout-note collapse="true"}
# Cases where BANKSY may not work as well

For example, if we assess this Colorectal Cancer sample from [Oliveira, M., et al. Nat Genetics (2025)](https://www.nature.com/articles/s41588-025-02193-3), we see that there is quite a bit of striation in the bottom left of the slide when we visualize the nUMIs for the dataset.

<img src="../img/crc_nUMI.png" width="300"/>

This would indicate to use that there is a population of a unique celltype (in this case tumor cells) that are interspersed with other celltypes. Essentially, the tumor cells do not form a condensed block on the tissue. When comparing the results from running BANKSY with `resolution = 0.8` this piece of information is lost. Conversely, the results from `FindClusters()` appear to retain the structure of these celltypes.

<img src="../img/crc_banksy_clusters.png" width="800"/>

In cases like this, either setting a more lenient lambda value or foregoing the use of BANKSY would best represent the biological signal. It is important to center your analysis choices based upon the biological question that is being asked.

:::

# Cell Type Annotation

The next step in the workflow is to annotate our dataset. In particular, we are interested in the cortical layers of our dataset.

<p align="center">

<img src="../img/Cell_type_annotations.png" width="800"/>

</p>

We will first subset our Seurat object to the region of interest. There are multiple ways of subsetting a Seurat object, but here we have identified a handful of cluster numbers that appear almost exclusively in the cortical region, and we will subset the object to only include cells that are assigned these cluster numbers.

```{r}
# Subset object_filt to bins in cluster 18, 19, 7, 2, 4
cortex <- subset(object_filt, seurat_cluster.projected %in% c(18, 19, 7, 2, 4))

# Create unique color palette
color_pal <- Seurat::DiscretePalette(n = length(unique(object_filt$seurat_cluster.projected)),
                                    palette = "polychrome")
names(color_pal) <- sort(unique(object_filt$seurat_cluster.projected))

# Plot clusters (cortical layers only)
SpatialDimPlot(cortex, group.by = 'seurat_cluster.projected', 
               pt.size.factor = 8, cols = color_pal)
```

::: callout-note
# Differences in plots

Your colors may be different than the ones in the above figure.
:::

To perform accurate annotation of cell types, we must also take into consideration that our 16µm x 16µm bins may contain multiple cells. The method [Robust Cell Type Deconvolution](https://www.nature.com/articles/s41587-021-00830-w) (RCTD) has been shown to accurately annotate spatial data from a variety of technologies while taking into consideration that a single bin may exhibit multiple cell type profiles.

RCTD takes a cell-type-annotated scRNA-seq dataset as a reference and a spatial dataset as a query. For our reference, we will use a subsampled version of the mouse scRNA-seq dataset from the [Allen Brain Atlas](https://mouse.brain-map.org/static/atlas). We will use our `cortex` Seurat object as the spatial query. As an overview, the process is as follows:

1.  Sketch and process the spatial query dataset
2.  Load and format the scRNA-seq reference dataset
3.  Apply RCTD to deconvolute the "sketched" cortical cells and annotate them
4.  Project these annotations onto the full cortical dataset.

## Sketch and process the spatial query dataset

Since we have subset the dataset, we now need to re-run the typical scRNA workflow. Without the rest of the bins, we should be able to recognize more subtle shifts in the cortex bins. Therefore, we need re-calculate the variable genes and following steps to clearly delineate the layers.

```{r}
# Create sketch of cortex subset
DefaultAssay(cortex) <- 'Spatial.016um'
cortex <- FindVariableFeatures(cortex)
cortex <- SketchData(
  object = cortex,
  ncells = 3000,
  method = "LeverageScore",
  sketched.assay = "sketch"
)

# Run through scRNA workflow
DefaultAssay(cortex) <- "sketch"
cortex <- ScaleData(cortex)
cortex <- RunPCA(cortex, assay = "sketch", 
                 reduction.name = "pca.cortex.sketch", verbose = T)
cortex <- FindNeighbors(cortex, reduction = "pca.cortex.sketch", dims = 1:50)
cortex <- RunUMAP(cortex, reduction = "pca.cortex.sketch", 
                  reduction.name = "umap.cortex.sketch", return.model = T, 
                  dims = 1:50, verbose = T)
```

RCTD requires a unique data structure (similar to Seurat) as input to run. So we create our query object with `SpatialRNA()` by suppling the spatial coordinates and raw counts for the bins.

```{r}
# Grab barcodes for sketched bins
counts_hd <- cortex[["sketch"]]$counts
cortex_cells_hd <- colnames(cortex[["sketch"]])

# Spatial coordinates for each bin
coords <- GetTissueCoordinates(cortex)[cortex_cells_hd, 1:2]

# Create the RCTD query object
query <- SpatialRNA(coords, counts_hd, colSums(counts_hd))
```

## Load and format the reference dataset

After loading our reference dataset, we can take a quick look at the different celltypes that we are going to annotate our query object with. For this reference, these annotation values are stored in the column `subclass_label`. Note that this reference dataset was downsampled for ease of loading onto a laptop.

```{r}
# Increase amount of memory R can use
mem.maxVSize(15000)

# Load Allen Brain reference
ref_subset <- qread("data_processed/allen_scRNAseq_ref_subset.qs")

# Plot number of cells per celltype
ggplot(ref_subset@meta.data) +
  geom_bar(aes(x = subclass_label, 
               fill = subclass_label)) +
  theme_bw() + NoLegend() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

We are going to make another RCTD object out of the reference dataset. This time we use the `Reference()` function and additionally supply `cluster` (celltype) information to transfer to our query.

```{r}
# Raw counts of reference dataset
counts <- ref_subset[["RNA"]]$counts

# Celltype annotation of reference dataset
Idents(ref_subset) <- "subclass_label"
cluster <- as.factor(ref_subset$subclass_label)
levels(cluster) <- gsub("/", "-", levels(cluster))
cluster <- droplevels(cluster)

# Raw counts of reference dataset
nUMI <- ref_subset$nCount_RNA

# Create the RCTD reference object
reference <- Reference(counts, cluster, nUMI)
```

## Apply RCTD to deconvolute the "sketched" cortical cells and annotate them

Note that `run.RCTD` takes 10-15 minutes to complete on a laptop using 6 cores.

```{r}
#| eval: false
# Run RCTD
RCTD <- create.RCTD(query, reference, max_cores = 6)
RCTD <- run.RCTD(RCTD, doublet_mode = "doublet") # this command takes ~15 mins to run

# Add results back to Seurat object
cortex <- AddMetaData(cortex, metadata = RCTD@results$results_df)
```

```{r}
#| label: load_cortex_RCTD
#| echo: false
# saveRDS(cortex, "results/cortex_RCTD.rds")
cortex <- readRDS("data_processed/cortex_RCTD.rds")
```

::: {.callout-note collapse="true"}
# RCTD output

The resultant dataframe from RCTD will contain the following columns according to the [documentation](https://mirror.accum.se/mirror/bioconductor.org/packages/devel/bioc/vignettes/spacexr/inst/doc/rctd-tutorial.html):

-   `spot_class`, a factor variable representing RCTD's classification in doublet mode:
    -   "singlet" (1 cell type on pixel)
    -   "doublet_certain" (2 cell types on pixel)
    -   "doublet_uncertain" (2 cell types on pixel, but only confident of 1)
    -   "reject" (no prediction given for pixel)
-   `first_type` column gives the first cell type predicted on the bead (for all spot_class conditions except "reject").
-   `second_type` column gives the second cell type predicted on the bead for doublet spot_class conditions (not a confident prediction for "doublet_uncertain").

Which we can access from our `cortex` object:

```{r}
#| eval: false
# Columns of interest
cols <- c("spot_class", "first_type", "second_type")

# View columns of interest (ignore NA values)
cortex@meta.data[cols] %>%
  subset(!is.na(spot_class)) %>%
  View()
```

```{r}
#| label: dt_cortex_RCTD
#| echo: false
library(DT)
cols <- c("spot_class", "first_type", "second_type")
datatable(cortex@meta.data[cols] %>%
  subset(!is.na(spot_class)) %>% 
    head(5)
)
```
:::

## Project RCTD labels onto all cortical cells

We once again use the `ProjectData()` function in order to annotate the full dataset, not just the sketch. These values will be stored in a new column named `full_first_type`

```{r}
# Set all NA values to "Unknown"
cortex$first_type <- as.character(cortex$first_type)
cortex$first_type[is.na(cortex$first_type)] <- "Unknown"

# Project back first_type to new column called full_first_type
cortex <- ProjectData(
  object = cortex,
  assay = "Spatial.016um",
  full.reduction = "pca.cortex",
  sketched.assay = "sketch",
  sketched.reduction = "pca.cortex.sketch",
  umap.model = "umap.cortex.sketch",
  dims = 1:50,
  refdata = list(full_first_type = "first_type")
)
```

To visualize the cortical layers, we first identify all the cells that belong to each annotation with `CellsByIdentities()`. Then, we select the layered neurons of interest using regular expressions. We do this so that when we visualize these cells later one, we can highly the cells of each layer clearly.

```{r}
# Identify bins by celltype
Idents(cortex) <- "full_first_type"
cells <- CellsByIdentities(cortex)

# Excitatory neurons in the cortex are all labeled with an L* at the start
excitatory_names <- sort(grep("^L.* CTX", names(cells), value = TRUE))
excitatory_names
```

We can see that the excitatory neurons are located in layers at varying cortical depths, as expected.

```{r}
# Plot each cortical layer separately
SpatialDimPlot(cortex, cells.highlight = cells[excitatory_names], 
               cols.highlight = c("#FFFF00", "grey50"), facet.highlight = T, 
               combine = T, ncol = 4, pt.size.factor = 8)
```

# Save R Session

Remember to save your session information at the end of the analysis! This will let future users know which versions of packages are necessary in order to reproduce the same results.

```{r}
#| eval: false
# Create and save a text file with sessionInfo
sink("results/sessionInfo_visiumHD.txt")
sessionInfo()
sink()
```
